{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Benchmark Utility in PyLops-MPI\nThis tutorial demonstrates how to use the :py:func:`pylops_mpi.utils.benchmark` and\n:py:func:`pylops_mpi.utils.mark` utility methods in PyLops-MPI. It contains various\nfunction calling pattern that may come up during the benchmarking of a distributed code.\n\n:py:func:`pylops_mpi.utils.benchmark` is a decorator used to decorate any\nfunction to measure its execution time from start to finish\n:py:func:`pylops_mpi.utils.mark` is a function used inside the benchmark-decorated\nfunction to provide fine-grain time measurements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sys\nimport logging\nimport numpy as np\nfrom mpi4py import MPI\nfrom pylops_mpi import DistributedArray, Partition\n\nnp.random.seed(42)\nrank = MPI.COMM_WORLD.Get_rank()\n\npar = {'global_shape': (500, 501),\n       'partition': Partition.SCATTER, 'dtype': np.float64,\n       'axis': 1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's start by import the utility and a simple exampple\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pylops_mpi.utils.benchmark import benchmark, mark\n\n\n@benchmark\ndef inner_func(par):\n    dist_arr = DistributedArray(global_shape=par['global_shape'],\n                                partition=par['partition'],\n                                dtype=par['dtype'], axis=par['axis'])\n    # may perform computation here\n    dist_arr.dot(dist_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we call :py:func:`inner_func`, we will see the result\nof the benchmark print to standard output. If we want to customize the\nfunction name in the printout, we can pass the parameter `description`\nto the :py:func:`benchmark`\ni.e., :py:func:`@benchmark(description=\"printout_name\")`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inner_func(par)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We may want to get the fine-grained time measurements by timing the execution\ntime of arbitary lines of code. :py:func:`pylops_mpi.utils.mark` provides such utitlity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@benchmark\ndef inner_func_with_mark(par):\n    mark(\"Begin array constructor\")\n    dist_arr = DistributedArray(global_shape=par['global_shape'],\n                                partition=par['partition'],\n                                dtype=par['dtype'], axis=par['axis'])\n    mark(\"Begin dot\")\n    dist_arr.dot(dist_arr)\n    mark(\"Finish dot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now when we run, we get the detailed time measurement. Note that there is a tag\n[decorator] next to the function name to distinguish between the start-to-end time\nmeasurement of the top-level function and those that comes from :py:func:`pylops_mpi.utils.mark`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inner_func_with_mark(par)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This utility benchmarking routines can also be nested. Let's define\nan outer function that internally calls the decorated :py:func:`inner_func_with_mark`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@benchmark\ndef outer_func_with_mark(par):\n    mark(\"Outer func start\")\n    inner_func_with_mark(par)\n    dist_arr = DistributedArray(global_shape=par['global_shape'],\n                                partition=par['partition'],\n                                dtype=par['dtype'], axis=par['axis'])\n    dist_arr + dist_arr\n    mark(\"Outer func ends\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we run :py:func:`outer_func_with_mark`, we get the time measurement nicely\nprinted out with the nested indentation to specify that nested calls.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "outer_func_with_mark(par)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In some cases, we may want to write benchmark output to a text file.\n:py:func:`pylops_mpi.utils.benchmark` also takes the py:class:`logging.Logger`\nin its argument.\nHere we define a simple :py:func:`make_logger()`. We set the :py:func:`logger.propagate = False`\nto isolate the logging of our benchmark from that of the rest of the code\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "save_file = True\nfile_path = \"benchmark.log\"\n\n\ndef make_logger(save_file=False, file_path=''):\n    logger = logging.getLogger(__name__)\n    logging.basicConfig(filename=file_path if save_file else None, filemode='w', level=logging.INFO, force=True)\n    logger.propagate = False\n    if save_file:\n        handler = logging.FileHandler(file_path, mode='w')\n    else:\n        handler = logging.StreamHandler(sys.stdout)\n    logger.addHandler(handler)\n    return logger\n\n\nlogger = make_logger(save_file, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we can pass the logger to the :py:func:`pylops_mpi.utils.benchmark`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@benchmark(logger=logger)\ndef inner_func_with_logger(par):\n    dist_arr = DistributedArray(global_shape=par['global_shape'],\n                                partition=par['partition'],\n                                dtype=par['dtype'], axis=par['axis'])\n    # may perform computation here\n    dist_arr.dot(dist_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run this function and observe that the file `benchmark.log` is written.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inner_func_with_logger(par)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}